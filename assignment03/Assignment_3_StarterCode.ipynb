{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbupO2bS52ju"
      },
      "source": [
        "# **Machine Learning for Neuroimaging 2024 ‚Äî Assignment 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6k_SV9veBII"
      },
      "source": [
        "## **ABIDE I Functional Connectome Analysis**\n",
        "\n",
        "You have been provided with a resting-state fMRI connectome dataset of 120 individuals diagnosed with Autism Spectrum Disorder (ASD) and 120 typical controls. Each connectome, i.e., each subject‚Äôs brain network and properties, is encoded as an ùëÅ-by-ùëÅ correlation matrix ùêå, where $M_{i,j}$ is the functional connectivity (correlation in activation patterns) between region $i$ and region $j$.\n",
        "\n",
        "Note that this is the preprocessed version of ABIDE provided by the Preprocessed Connectome Project (PCP).\n",
        "\n",
        "For more information about this dataset's structure: http://preprocessed-connectomes-project.org/abide/\n",
        "\n",
        "*Cameron Craddock, Yassine Benhajali, Carlton Chu, Francois Chouinard, Alan Evans, Andr√°s Jakab, Budhachandra Singh Khundrakpam, John David Lewis, Qingyang Li, Michael Milham, Chaogan Yan, Pierre Bellec (2013). The Neuro Bureau Preprocessing Initiative: open sharing of preprocessed neuroimaging data and derivatives. In Neuroinformatics 2013, Stockholm, Sweden.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPUs4Vsp6sap"
      },
      "source": [
        "**Relevant Libraries**\n",
        "*  [NumPy](https://numpy.org/):https For numerical computing and handling multi-dimensional data.\n",
        "*  [Pandas](https://pandas.pydata.org/): For structured data operations and manipulations.\n",
        "*  [Matplotlib](https://matplotlib.org/): For creating static, interactive, and animated visualizations in Python.\n",
        "*  [scikit-learn](https://scikit-learn.org/stable/): For implementing machine learning algorithms.\n",
        "*  [nibabel](https://nipy.org/nibabel/): For reading and writing neuroimaging data formats.\n",
        "*  [nilearn](https://nilearn.github.io/stable/index.html): For advanced neuroimaging data manipulation and visualization.\n",
        "*  [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/): For writing and training Graph Neural Networks (GNNs).\n",
        "*  [DGL (Deep Graph Library)](https://www.dgl.ai/): For deep learning on GNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H2ljMMms9tqg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# @title Run to install needed packages.\n",
        "!pip install nilearn torch torchvision torchaudio torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVOPS-V3tYHZ"
      },
      "outputs": [],
      "source": [
        "# @title Construct weighted graphs using our connectivity matrices\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "from nilearn.connectome import ConnectivityMeasure\n",
        "\n",
        "'''\n",
        "Load the connectivity matrices\n",
        "Note: each connectivity matrix (sample) was derived using the NiftiLabelsMasker\n",
        "and ConnectivityMeasure [with full Pearson's correlation] from nilearn and each\n",
        "sample was also standardized by z-score\n",
        "[i.e., zero mean scaled to unit variance w.r.t. sample std]\n",
        "'''\n",
        "\n",
        "data_path = './ABIDE_240.npz' # Be sure to update this to wherever you stored data\n",
        "\n",
        "# Load the data\n",
        "data = np.load(data_path)\n",
        "\n",
        "# Accessing the arrays\n",
        "connectomes = data['features']\n",
        "labels_abide = data['labels']\n",
        "\n",
        "print(\"ABIDE data loaded from NPZ file.\")\n",
        "\n",
        "# Check the number of subject functional scans fetched\n",
        "print(f\"Number of participants: {len(connectomes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-s2tWmyfpIX"
      },
      "outputs": [],
      "source": [
        "from nilearn import plotting\n",
        "from nilearn import datasets\n",
        "\n",
        "# Retrieve AAL brain atlas for parcellation, more info here: https://www.sciencedirect.com/science/article/pii/S1053811901909784\n",
        "parcellations = datasets.fetch_atlas_aal()\n",
        "atlas_filename = parcellations.maps\n",
        "labels_parc = parcellations.labels\n",
        "print(f\"Number of ROIs: {len(labels_parc)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4."
      ],
      "metadata": {
        "id": "EVLviv8C5mje"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E4jpX10nFoW"
      },
      "source": [
        "Plot a random connectivity matrix from the dataset.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpREL05C4S7_"
      },
      "outputs": [],
      "source": [
        "cm_sample = connectomes[47]\n",
        "print(f\"Connectivity matrix shape: {cm_sample.shape}\")\n",
        "\n",
        "np.fill_diagonal(cm_sample, 0)\n",
        "\n",
        "'''\n",
        "Note that that each \"pixel\" in the image is a brain region of interest (ROI)\n",
        "and the color denotes the strength (</>) and direction (+/-) of the correlation\n",
        " between any two ROIs.\n",
        "'''\n",
        "plotting.plot_matrix(cm_sample, figure=(6, 6),\n",
        "                     labels=range(cm_sample.shape[-1]),\n",
        "                     vmax=0.8, vmin=-0.8, reorder=False,\n",
        "                     title=f'Correlation matrix for d={cm_sample.shape[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot glass brain plot of the `cm_sample`.\n",
        "\n"
      ],
      "metadata": {
        "id": "qfHJq0e1LuFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab center coordinates for atlas labels\n",
        "coordinates = plotting.find_parcellation_cut_coords(labels_img=atlas_filename)\n",
        "\n",
        "# plot glass brain with labels\n",
        "plotting.plot_connectome(cm_sample,\n",
        "                         coordinates,\n",
        "                         edge_threshold=\"60%\", # thresholded due to high density\n",
        "                         title=\"AAL Atlas (func)\"\n",
        "                        )"
      ],
      "metadata": {
        "id": "Nt3Vh5a5LsuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHeuPmXuuOG-"
      },
      "source": [
        "The labels contain diagnostic group each participant is in. It is coded as:\n",
        "\n",
        "*   1 = Autism Spectrum Disorder (ASD)\n",
        "*   2 = Normal Control (NC)\n",
        "\n",
        "Let's re-index this to `0=ASD` and `1=NC` due to zero-indexed systems required for ML and other data processing software."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCuvTsu1uWgc"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Adjust labels to start from 0; 0=ASD, 1=NC\n",
        "y_abide = labels_abide - 1\n",
        "\n",
        "# Print class labels and counts\n",
        "print(\"Class distribution:\", [f\"({key}: {value})\" for key, value in Counter(y_abide).items()])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate connectomes into subgroups for any comparative analyses\n",
        "connectomes_asd = connectomes[y_abide == 0]\n",
        "connectomes_nc = connectomes[y_abide == 1]"
      ],
      "metadata": {
        "id": "xA6gCNCgXjSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDt1B2c_LU8Q"
      },
      "source": [
        "Now, you're ready to use the data for machine learning or statistical analysis.\n",
        "\n",
        "Note: If you are interested in using graph neural networks, feel free to use the below PyG starter code to create your custom PyG Dataset. Check the assignment description for links to related tutorials.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hetxZvDwM8iR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title [GNNs ONLY] Create custom PyG ConnectomeDataset\n",
        "import torch\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "\n",
        "class ConnectomeDataset(Dataset):\n",
        "    def __init__(self, connectivity_matrices, labels, task=\"classification\", transform=None, pre_transform=None):\n",
        "        super(ConnectomeDataset, self).__init__(None, transform, pre_transform)\n",
        "        self.connectivity_matrices = connectivity_matrices\n",
        "        self.labels = labels\n",
        "        self.task = task\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.connectivity_matrices)\n",
        "\n",
        "    def get(self, idx):\n",
        "        # Convert the connectivity matrix to edge index and edge attributes\n",
        "        connectivity_matrix = torch.tensor(self.connectivity_matrices[idx])\n",
        "        edge_index, edge_attr = dense_to_sparse(connectivity_matrix)\n",
        "\n",
        "        # Create a data object\n",
        "        data = Data(edge_index=edge_index, edge_attr=edge_attr)\n",
        "        data.x = connectivity_matrix.to(torch.float)\n",
        "        if self.task == \"classification\":\n",
        "          data.y = torch.tensor([self.labels[idx]], dtype=torch.long)\n",
        "        else:\n",
        "          data.y = torch.tensor([self.labels[idx]], dtype=torch.float)\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example - GNN Classification"
      ],
      "metadata": {
        "id": "dXX-GuaFU5NY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB1Fd0CQNTRt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_geometric.transforms as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Instantiate the dataset\n",
        "abide_dataset = ConnectomeDataset(connectomes, y_abide)\n",
        "\n",
        "loader = DataLoader(abide_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(f'Number of graphs: {len(abide_dataset)}')\n",
        "print(\"Number of node features: \", abide_dataset.num_node_features)\n",
        "print(f'Number of edge features: {abide_dataset.num_edge_features}')\n",
        "\n",
        "num_classes = 2"
      ]
    },
    {
      "source": [
        "# Define a Graph Neural Network model.\n",
        "# For more tutorials and examples: https://pytorch-geometric.readthedocs.io/en/2.6.0/get_started/colabs.html\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(abide_dataset.num_node_features, hidden_channels)\n",
        "\n",
        "        ###### Your code here ######\n",
        "        ## 1. Add more convolutional layers and a final linear layer\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        ###### Your code here ######\n",
        "        # 1. Obtain node embeddings\n",
        "        x = ...\n",
        "\n",
        "\n",
        "        # 2. Readout layer (using a global aggregation function e.g. global_mean_pool)\n",
        "        x = ...\n",
        "\n",
        "        # 3. Apply a final classifier (be sure to implement dropout with self.training)\n",
        "        x = ...\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NhJrodOq-YpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Implement your training function\n",
        "def train(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "\n",
        "    loss = 0\n",
        "    ###### Your code here ######\n",
        "    ## 1. Be sure to at least return the loss and accuracy.\n",
        "\n",
        "\n",
        "    # Return the loss and accuracy\n",
        "    return loss, accuracy\n",
        "\n",
        "# Implement your test (evaluation) function\n",
        "def test(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    ###### Your code here ######\n",
        "    ## 1. Return test loss, accuracy, precision, recall and F-1 score.\n",
        "\n",
        "\n",
        "    # Return the loss, accuracy, precision, recall and F-1 score.\n",
        "    return loss / len(loader), accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "1hZHTTXwASX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available and will be used.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available, using CPU instead.\")\n",
        "\n",
        "# Define the number of folds\n",
        "num_folds = ...   ###### Your code here ######\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = ...   ###### Your code here ######\n",
        "\n",
        "# Initialize StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)  # Set random_state for reproducibility\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "test_losses = []\n",
        "test_accs = []\n",
        "test_precs = []\n",
        "test_recs = []\n",
        "test_f1s = []\n",
        "\n",
        "# Loop through the folds\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(connectomes, y_abide)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "\n",
        "    # Create train and test datasets for the current fold\n",
        "    train_dataset = torch.utils.data.Subset(abide_dataset, train_index)\n",
        "    test_dataset = torch.utils.data.Subset(abide_dataset, test_index)\n",
        "\n",
        "    # Create DataLoaders for the current fold\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)  # No need to shuffle test data\n",
        "\n",
        "    # Instantiate your GNN model here\n",
        "    model = GCN(hidden_channels=64).to(device)  # Replace with your model definition; move to GPU if available\n",
        "\n",
        "    # Define optimizer and criterion\n",
        "    optimizer = ...  ###### Your code here ######\n",
        "    criterion = ...  ###### Your code here ######\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, ' +\n",
        "        f'Train Accuracy: {train_acc:.4f}')\n",
        "\n",
        "    # Evaluation on the test set\n",
        "    test_loss, test_acc, test_prec, test_rec, test_f1 = test(model, test_loader, criterion, device)\n",
        "    print(test_loss, test_acc, test_prec, test_rec, test_f1)\n",
        "    print(f'Test | Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f} ' +\n",
        "          f'Precision: {test_prec:.4f}, Recall: {test_rec:.4f} ' +\n",
        "          f'F1: {test_f1:.4f}')\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accs.append(test_acc)\n",
        "    test_precs.append(test_prec)\n",
        "    test_recs.append(test_rec)\n",
        "    test_f1s.append(test_f1)\n",
        "\n",
        "  # After training and evaluation, print the average results.\n",
        "  ###### Your code here ######\n",
        "\n"
      ],
      "metadata": {
        "id": "g-m8AjbMADzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5."
      ],
      "metadata": {
        "id": "qtgTMnc_JFGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example - Graph Theory Metrics"
      ],
      "metadata": {
        "id": "U7VfsCZRJiEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 'cm_sample' is a random connectome from our dataset\n",
        "weights = cm_sample[np.triu_indices_from(cm_sample, k=1)]  # Extract upper triangle to avoid duplication\n",
        "\n",
        "# Plotting the distribution of weights\n",
        "plt.hist(weights, bins=30, alpha=0.7)\n",
        "plt.title(\"Distribution of Edge Weights\")\n",
        "plt.xlabel(\"Weight (Correlation Value)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0PoKW_nsJEZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example - Network Community Detection"
      ],
      "metadata": {
        "id": "fBBah-dJLUL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import community.community_louvain as community_louvain\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib import colormaps\n",
        "\n",
        "'''Note: networkx expects non-negative edge weights for community detection;\n",
        "convert connectome edge weights to absolute value (changes edge weight\n",
        "information to strength only and not whether positively or negatively\n",
        "correlated.)\n",
        "'''\n",
        "# Find the absolute maximum value in the connectome for scaling\n",
        "abs_cm_sample = np.abs(cm_sample)\n",
        "\n",
        "G = nx.from_numpy_array(abs_cm_sample)\n",
        "\n",
        "# Use the Louvain method for community detection\n",
        "partition = community_louvain.best_partition(G, weight='weight')\n",
        "\n",
        "# Visualize the communities\n",
        "pos = nx.spring_layout(G)  # Positioning of the nodes\n",
        "cmap = colormaps['viridis']\n",
        "\n",
        "for com in set(partition.values()):\n",
        "    list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
        "    nx.draw_networkx_nodes(G, pos, list_nodes, node_size=20,\n",
        "                           node_color=[cmap(com / (max(partition.values()) + 1))])\n",
        "\n",
        "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FpuWInrRLcYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example - Local Clustering Coefficient"
      ],
      "metadata": {
        "id": "DsbenRG2TlgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Calculate local clustering coefficients for connectomes_asd\n",
        "local_clustering_asd = []\n",
        "for connectome in connectomes_asd:\n",
        "    # Create a NetworkX graph from the correlation matrix\n",
        "    G = nx.from_numpy_array(connectome)\n",
        "    # Calculate local clustering coefficients for all nodes\n",
        "    local_coeffs = nx.clustering(G)\n",
        "    # Append the average local clustering coefficient to the list\n",
        "    local_clustering_asd.append(np.mean(list(local_coeffs.values())))\n",
        "\n",
        "# Calculate local clustering coefficients for connectomes_nc\n",
        "local_clustering_nc = []\n",
        "for connectome in connectomes_nc:\n",
        "    # Create a NetworkX graph from the correlation matrix\n",
        "    G = nx.from_numpy_array(connectome)\n",
        "    # Calculate local clustering coefficients for all nodes\n",
        "    local_coeffs = nx.clustering(G)\n",
        "    # Append the average local clustering coefficient to the list\n",
        "    local_clustering_nc.append(np.mean(list(local_coeffs.values())))\n",
        "\n",
        "# Compare the average local clustering coefficients\n",
        "avg_local_clustering_asd = np.mean(local_clustering_asd)\n",
        "avg_local_clustering_nc = np.mean(local_clustering_nc)\n",
        "\n",
        "print(f\"Average Local Clustering Coefficient for ASD: {avg_local_clustering_asd:.4f}\")\n",
        "print(f\"Average Local Clustering Coefficient for NC: {avg_local_clustering_nc:.4f}\")\n",
        "\n",
        "## Compare the subgroups\n",
        "# Perform independent samples t-test\n",
        "t_statistic, p_value = stats.ttest_ind(clustering_coeffs_asd, clustering_coeffs_nc)\n",
        "\n",
        "print(f\"T-statistic: {t_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Interpret the results\n",
        "if p_value < 0.05:\n",
        "    print(\"There is a statistically significant difference in local clustering coefficients between ASD and NC.\")\n",
        "else:\n",
        "    print(\"There is no statistically significant difference in local clustering coefficients between ASD and NC.\")"
      ],
      "metadata": {
        "id": "i2ee09r_dp30"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}